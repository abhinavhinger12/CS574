{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Update: changing the batch_size from 32 to 128 when warm up the model, adding Kappa loss from this kernel: https://www.kaggle.com/christofhenkel/weighted-kappa-loss-for-keras-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aptos2019-blindness-detection', 'resnet50']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, fbeta_score, cohen_kappa_score, accuracy_score\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "WORKERS = 2\n",
    "CHANNEL = 3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 300\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f62e848cd68>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "df_test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "\n",
    "x = df_train['id_code']\n",
    "y = df_train['diagnosis']\n",
    "\n",
    "x, y = shuffle(x, y, random_state=8)\n",
    "y.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_multi_label(target):\n",
    "#     multi_label = np.zeros((len(target), NUM_CLASSES))\n",
    "#     for i in range(len(target)):\n",
    "#         j = target[i] + 1\n",
    "#         multi_label[i][:j] = 1\n",
    "#     return np.array(multi_label)\n",
    "# multi_y = to_multi_label(y)\n",
    "# for j in range(5):\n",
    "#     print('original: ', y[j])\n",
    "#     print('multi-label: ', multi_y[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.legacy import interfaces\n",
    "from keras.optimizers import Optimizer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class AdamAccumulate_v1(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., amsgrad=False, accum_iters=20, **kwargs):\n",
    "        super(AdamAccumulate, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.effective_iterations = K.variable(0, dtype='int64', name='effective_iterations')\n",
    "\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "        self.accum_iters = K.variable(accum_iters, dtype='int64')\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "\n",
    "        self.updates = [K.update(self.iterations, self.iterations + 1)]\n",
    "\n",
    "        flag = K.equal(self.iterations % self.accum_iters, self.accum_iters - 1)\n",
    "        flag = K.cast(flag, K.floatx())\n",
    "\n",
    "        self.updates.append(K.update(self.effective_iterations,\n",
    "                                     self.effective_iterations + K.cast(flag, 'int64')))\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.effective_iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.effective_iterations, K.floatx()) + 1\n",
    "\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        gs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat, gg in zip(params, grads, ms, vs, vhats, gs):\n",
    "\n",
    "            gg_t = (1 - flag) * (gg + g)\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * (gg + flag * g) / K.cast(self.accum_iters, K.floatx())\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(\n",
    "                (gg + flag * g) / K.cast(self.accum_iters, K.floatx()))\n",
    "\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - flag * lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                p_t = p - flag * lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append((m, flag * m_t + (1 - flag) * m))\n",
    "            self.updates.append((v, flag * v_t + (1 - flag) * v))\n",
    "            self.updates.append((gg, gg_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad}\n",
    "        base_config = super(AdamAccumulate, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class AdamAccumulate(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., amsgrad=False, accum_iters=2, **kwargs):\n",
    "        if accum_iters < 1:\n",
    "            raise ValueError('accum_iters must be >= 1')\n",
    "        super(AdamAccumulate, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "        self.accum_iters = K.variable(accum_iters, K.dtype(self.iterations))\n",
    "        self.accum_iters_float = K.cast(self.accum_iters, K.floatx())\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "\n",
    "        completed_updates = K.cast(K.tf.floor(self.iterations / self.accum_iters), K.floatx())\n",
    "\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * completed_updates))\n",
    "\n",
    "        t = completed_updates + 1\n",
    "\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        # self.iterations incremented after processing a batch\n",
    "        # batch:              1 2 3 4 5 6 7 8 9\n",
    "        # self.iterations:    0 1 2 3 4 5 6 7 8\n",
    "        # update_switch = 1:        x       x    (if accum_iters=4)\n",
    "        update_switch = K.equal((self.iterations + 1) % self.accum_iters, 0)\n",
    "        update_switch = K.cast(update_switch, K.floatx())\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        gs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat, tg in zip(params, grads, ms, vs, vhats, gs):\n",
    "\n",
    "            sum_grad = tg + g\n",
    "            avg_grad = sum_grad / self.accum_iters_float\n",
    "\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * avg_grad\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(avg_grad)\n",
    "\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, (1 - update_switch) * vhat + update_switch * vhat_t))\n",
    "            else:\n",
    "                p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, (1 - update_switch) * m + update_switch * m_t))\n",
    "            self.updates.append(K.update(v, (1 - update_switch) * v + update_switch * v_t))\n",
    "            self.updates.append(K.update(tg, (1 - update_switch) * sum_grad))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, (1 - update_switch) * p + update_switch * new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad}\n",
    "        base_config = super(AdamAccumulate, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3112,)\n",
      "(3112, 5)\n",
      "(550,)\n",
      "(550, 5)\n"
     ]
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.15,\n",
    "                                                      stratify=y, random_state=8)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(valid_x.shape)\n",
    "print(valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/aleju/imgaug\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "seq = iaa.Sequential([\n",
    "    sometimes(\n",
    "        iaa.OneOf([\n",
    "            iaa.Add((-10, 10), per_channel=0.5),\n",
    "            iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
    "            iaa.ContrastNormalization((0.9, 1.1), per_channel=0.5)\n",
    "        ])\n",
    "    ),\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Crop(percent=(0, 0.1)),\n",
    "    iaa.Flipud(0.5)\n",
    "],random_order=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Generator(Sequence):\n",
    "\n",
    "    def __init__(self, image_filenames, labels,\n",
    "                 batch_size, is_train=False,\n",
    "                 mix=False, augment=False):\n",
    "        self.image_filenames, self.labels = image_filenames, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.is_train = is_train\n",
    "        self.is_augment = augment\n",
    "        if(self.is_train):\n",
    "            self.on_epoch_end()\n",
    "        self.is_mix = mix\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        if(self.is_train):\n",
    "            return self.train_generate(batch_x, batch_y)\n",
    "        return self.valid_generate(batch_x, batch_y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if(self.is_train):\n",
    "            self.image_filenames, self.labels = shuffle(self.image_filenames, self.labels)\n",
    "    \n",
    "    def mix_up(self, x, y):\n",
    "        lam = np.random.beta(0.2, 0.4)\n",
    "        ori_index = np.arange(int(len(x)))\n",
    "        index_array = np.arange(int(len(x)))\n",
    "        np.random.shuffle(index_array)        \n",
    "        \n",
    "        mixed_x = lam * x[ori_index] + (1 - lam) * x[index_array]\n",
    "        mixed_y = lam * y[ori_index] + (1 - lam) * y[index_array]\n",
    "        \n",
    "        return mixed_x, mixed_y\n",
    "\n",
    "    def train_generate(self, batch_x, batch_y):\n",
    "        batch_images = []\n",
    "        for (sample, label) in zip(batch_x, batch_y):\n",
    "            img = cv2.imread('../input/aptos2019-blindness-detection/train_images/'+sample+'.png')\n",
    "            img = cv2.resize(img, (SIZE, SIZE))\n",
    "            if(self.is_augment):\n",
    "                img = seq.augment_image(img)\n",
    "            batch_images.append(img)\n",
    "        batch_images = np.array(batch_images, np.float32) / 255\n",
    "        batch_y = np.array(batch_y, np.float32)\n",
    "        if(self.is_mix):\n",
    "            batch_images, batch_y = self.mix_up(batch_images, batch_y)\n",
    "        return batch_images, batch_y\n",
    "\n",
    "    def valid_generate(self, batch_x, batch_y):\n",
    "        batch_images = []\n",
    "        for (sample, label) in zip(batch_x, batch_y):\n",
    "            img = cv2.imread('../input/aptos2019-blindness-detection/train_images/'+sample+'.png')\n",
    "            img = cv2.resize(img, (SIZE, SIZE))\n",
    "            batch_images.append(img)\n",
    "        batch_images = np.array(batch_images, np.float32) / 255\n",
    "        batch_y = np.array(batch_y, np.float32)\n",
    "        return batch_images, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D)\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = \"softmax\"\n",
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = ResNet50(include_top=False,\n",
    "                   weights=None,\n",
    "                   input_tensor=input_tensor)\n",
    "    base_model.load_weights('../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    final_output = Dense(n_out, activation=function, name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n",
    "                             EarlyStopping, ReduceLROnPlateau,CSVLogger)\n",
    "\n",
    "epochs = 30; batch_size = 32\n",
    "checkpoint = ModelCheckpoint('../working/Resnet50.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='min', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=9)\n",
    "\n",
    "csv_logger = CSVLogger(filename='../working/training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "# callbacks_list = [checkpoint, csv_logger, reduceLROnPlat, early]\n",
    "\n",
    "train_generator = My_Generator(train_x, train_y, 128, is_train=True)\n",
    "train_mixup = My_Generator(train_x, train_y, batch_size, is_train=True, mix=False, augment=True)\n",
    "valid_generator = My_Generator(valid_x, valid_y, batch_size, is_train=False)\n",
    "\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class QWKEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), batch_size=64, interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_generator, self.y_val = validation_data\n",
    "        self.history = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict_generator(generator=self.valid_generator,\n",
    "                                                  steps=np.ceil(float(len(self.y_val)) / float(self.batch_size)),\n",
    "                                                  workers=1, use_multiprocessing=True,\n",
    "                                                  verbose=1)\n",
    "            def flatten(y):\n",
    "                return np.argmax(y, axis=1).reshape(-1)\n",
    "                # return np.sum(y.astype(int), axis=1) - 1\n",
    "            \n",
    "            score = cohen_kappa_score(flatten(self.y_val),\n",
    "                                      flatten(y_pred),\n",
    "                                      labels=[0,1,2,3,4],\n",
    "                                      weights='quadratic')\n",
    "#             print(flatten(self.y_val)[:5])\n",
    "#             print(flatten(y_pred)[:5])\n",
    "            print(\"\\n epoch: %d - QWK_score: %.6f \\n\" % (epoch+1, score))\n",
    "            self.history.append(score)\n",
    "            if score >= max(self.history):\n",
    "                print('save checkpoint: ', score)\n",
    "                self.model.save('../working/Resnet50_bestqwk.h5')\n",
    "\n",
    "qwk = QWKEvaluation(validation_data=(valid_generator, valid_y),\n",
    "                    batch_size=batch_size, interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "25/25 [==============================] - 197s 8s/step - loss: 2.6028\n",
      "18/18 [==============================] - 102s 6s/step\n",
      "\n",
      " epoch: 1 - QWK_score: 0.000000 \n",
      "\n",
      "save checkpoint:  0.0\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 119s 5s/step - loss: 1.5631\n",
      "18/18 [==============================] - 97s 5s/step\n",
      "\n",
      " epoch: 2 - QWK_score: 0.000000 \n",
      "\n",
      "save checkpoint:  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f62e58e2ac8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# warm up model\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for i in range(-5,0):\n",
    "    model.layers[i].trainable = True\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    # loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-3))\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_y)) / float(128)),\n",
    "    epochs=2,\n",
    "    workers=WORKERS, use_multiprocessing=True,\n",
    "    verbose=1,\n",
    "    callbacks=[qwk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "98/98 [==============================] - 312s 3s/step - loss: 0.6618 - acc: 0.7506 - val_loss: 0.5223 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52231, saving model to ../working/Resnet50.h5\n",
      "18/18 [==============================] - 68s 4s/step\n",
      "\n",
      " epoch: 1 - QWK_score: 0.844133 \n",
      "\n",
      "save checkpoint:  0.844133051987517\n",
      "Epoch 2/30\n",
      "98/98 [==============================] - 260s 3s/step - loss: 0.5331 - acc: 0.8080 - val_loss: 0.4738 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52231 to 0.47379, saving model to ../working/Resnet50.h5\n",
      "18/18 [==============================] - 64s 4s/step\n",
      "\n",
      " epoch: 2 - QWK_score: 0.887789 \n",
      "\n",
      "save checkpoint:  0.887789394688117\n",
      "Epoch 3/30\n",
      "98/98 [==============================] - 266s 3s/step - loss: 0.4843 - acc: 0.8163 - val_loss: 0.4678 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.47379 to 0.46775, saving model to ../working/Resnet50.h5\n",
      "18/18 [==============================] - 66s 4s/step\n",
      "\n",
      " epoch: 3 - QWK_score: 0.866038 \n",
      "\n",
      "Epoch 4/30\n",
      "98/98 [==============================] - 259s 3s/step - loss: 0.4439 - acc: 0.8355 - val_loss: 0.4700 - val_acc: 0.8345\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.46775\n",
      "18/18 [==============================] - 67s 4s/step\n",
      "\n",
      " epoch: 4 - QWK_score: 0.889734 \n",
      "\n",
      "save checkpoint:  0.8897336549942735\n",
      "Epoch 5/30\n",
      "98/98 [==============================] - 273s 3s/step - loss: 0.4096 - acc: 0.8469 - val_loss: 0.5124 - val_acc: 0.8327\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.46775\n",
      "18/18 [==============================] - 67s 4s/step\n",
      "\n",
      " epoch: 5 - QWK_score: 0.885479 \n",
      "\n",
      "Epoch 6/30\n",
      "98/98 [==============================] - 260s 3s/step - loss: 0.3863 - acc: 0.8540 - val_loss: 0.4916 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.46775\n",
      "18/18 [==============================] - 67s 4s/step\n",
      "\n",
      " epoch: 6 - QWK_score: 0.862136 \n",
      "\n",
      "Epoch 7/30\n",
      "98/98 [==============================] - 274s 3s/step - loss: 0.3363 - acc: 0.8718 - val_loss: 0.5921 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.46775\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "18/18 [==============================] - 65s 4s/step\n",
      "\n",
      " epoch: 7 - QWK_score: 0.892953 \n",
      "\n",
      "save checkpoint:  0.892953312244815\n",
      "Epoch 8/30\n",
      "98/98 [==============================] - 263s 3s/step - loss: 0.3089 - acc: 0.8875 - val_loss: 0.5001 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.46775\n",
      "18/18 [==============================] - 66s 4s/step\n",
      "\n",
      " epoch: 8 - QWK_score: 0.887360 \n",
      "\n",
      "Epoch 9/30\n",
      "98/98 [==============================] - 276s 3s/step - loss: 0.2606 - acc: 0.9037 - val_loss: 0.5441 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.46775\n",
      "18/18 [==============================] - 67s 4s/step\n",
      "\n",
      " epoch: 9 - QWK_score: 0.881980 \n",
      "\n",
      "Epoch 10/30\n",
      "98/98 [==============================] - 260s 3s/step - loss: 0.2219 - acc: 0.9165 - val_loss: 0.5544 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.46775\n",
      "18/18 [==============================] - 65s 4s/step\n",
      "\n",
      " epoch: 10 - QWK_score: 0.897874 \n",
      "\n",
      "save checkpoint:  0.8978737145913435\n",
      "Epoch 11/30\n",
      "98/98 [==============================] - 267s 3s/step - loss: 0.2315 - acc: 0.9107 - val_loss: 0.5665 - val_acc: 0.8091\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.46775\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "18/18 [==============================] - 66s 4s/step\n",
      "\n",
      " epoch: 11 - QWK_score: 0.879125 \n",
      "\n",
      "Epoch 12/30\n",
      "98/98 [==============================] - 261s 3s/step - loss: 0.1917 - acc: 0.9340 - val_loss: 0.5767 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.46775\n",
      "18/18 [==============================] - 67s 4s/step\n",
      "\n",
      " epoch: 12 - QWK_score: 0.886860 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f62e58e2b38>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train all layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger, reduceLROnPlat, early, qwk]\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            # loss=kappa_loss,\n",
    "            # loss='binary_crossentropy',\n",
    "            # optimizer=Adam(lr=1e-4),\n",
    "            optimizer=AdamAccumulate(lr=1e-4, accum_iters=2),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(\n",
    "    train_mixup,\n",
    "    steps_per_epoch=np.ceil(float(len(train_x)) / float(batch_size)),\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_x)) / float(batch_size)),\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    workers=1, use_multiprocessing=False,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "# model.load_weights('../working/Resnet50.h5')\n",
    "model.load_weights('../working/Resnet50_bestqwk.h5')\n",
    "predicted = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1928it [01:24, 22.95it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, name in tqdm(enumerate(submit['id_code'])):\n",
    "    path = os.path.join('../input/aptos2019-blindness-detection/test_images/', name+'.png')\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, (SIZE, SIZE))\n",
    "    score_predict = model.predict((image[np.newaxis])/255)\n",
    "    label_predict = np.argmax(score_predict)\n",
    "    # label_predict = score_predict.astype(int).sum() - 1\n",
    "    predicted.append(str(label_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code diagnosis\n",
       "0  0005cfc8afb6         1\n",
       "1  003f0afdcd15         2\n",
       "2  006efc72b638         3\n",
       "3  00836aaacf06         2\n",
       "4  009245722fa4         3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['diagnosis'] = predicted\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "submit.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
